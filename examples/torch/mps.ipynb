{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python391jvsc74a57bd034fd1decc1a931af43cc0c05fabd64b4c8a56fc0949c2b3508ba9f0afa488d8e",
   "display_name": "Python 3.9.1 64-bit ('venv')"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '../../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "../../qmc/torch/mps/torchmps.py:496: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n  assert new_svs[i] is not -1\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import torch\n",
    "from qmc.torch.mps.torchmps import MPS\n",
    "from torchvision import transforms, datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Miscellaneous initialization\n",
    "torch.manual_seed(0)\n",
    "start_time = time.time()\n",
    "\n",
    "# MPS parameters\n",
    "bond_dim = 20\n",
    "adaptive_mode = False\n",
    "periodic_bc = False\n",
    "\n",
    "# Training parameters\n",
    "num_train = 2000\n",
    "num_test = 1000\n",
    "batch_size = 100\n",
    "num_epochs = 2\n",
    "learn_rate = 1e-4\n",
    "l2_reg = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the MPS module\n",
    "mps = MPS(\n",
    "    input_dim=28 ** 2,\n",
    "    output_dim=10,\n",
    "    bond_dim=bond_dim,\n",
    "    adaptive_mode=adaptive_mode,\n",
    "    periodic_bc=periodic_bc,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set our loss function and optimizer\n",
    "loss_fun = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(mps.parameters(), lr=learn_rate, weight_decay=l2_reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 503: Service Unavailable\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to ./mnist/MNIST/raw/train-images-idx3-ubyte.gz\n",
      "100.0%\n",
      "Extracting ./mnist/MNIST/raw/train-images-idx3-ubyte.gz to ./mnist/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 503: Service Unavailable\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to ./mnist/MNIST/raw/train-labels-idx1-ubyte.gz\n",
      "102.8%\n",
      "Extracting ./mnist/MNIST/raw/train-labels-idx1-ubyte.gz to ./mnist/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 503: Service Unavailable\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to ./mnist/MNIST/raw/t10k-images-idx3-ubyte.gz\n",
      "100.0%\n",
      "Extracting ./mnist/MNIST/raw/t10k-images-idx3-ubyte.gz to ./mnist/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 503: Service Unavailable\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to ./mnist/MNIST/raw/t10k-labels-idx1-ubyte.gz\n",
      "112.7%\n",
      "/Users/esgantivar/Documents/MISC/tmp/qmc/venv/lib/python3.9/site-packages/torchvision/datasets/mnist.py:502: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  ../torch/csrc/utils/tensor_numpy.cpp:143.)\n",
      "  return torch.from_numpy(parsed.astype(m[2], copy=False)).view(*s)\n",
      "Extracting ./mnist/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./mnist/MNIST/raw\n",
      "\n",
      "Processing...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# Get the training and test sets\n",
    "transform = transforms.ToTensor()\n",
    "train_set = datasets.MNIST(\"./mnist\", download=True, transform=transform)\n",
    "test_set = datasets.MNIST(\"./mnist\", download=True, transform=transform, train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put MNIST data into dataloaders\n",
    "samplers = {\n",
    "    \"train\": torch.utils.data.SubsetRandomSampler(range(num_train)),\n",
    "    \"test\": torch.utils.data.SubsetRandomSampler(range(num_test)),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaders = {\n",
    "    name: torch.utils.data.DataLoader(\n",
    "        dataset, batch_size=batch_size, sampler=samplers[name], drop_last=True\n",
    "    )\n",
    "    for (name, dataset) in [(\"train\", train_set), (\"test\", test_set)]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_batches = {\n",
    "    name: total_num // batch_size\n",
    "    for (name, total_num) in [(\"train\", num_train), (\"test\", num_test)]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "### Epoch 1 ###\n",
      "Average loss:           2.2708\n",
      "Average train accuracy: 0.1105\n",
      "### Epoch 2 ###\n",
      "Average loss:           1.8860\n",
      "Average train accuracy: 0.2780\n"
     ]
    }
   ],
   "source": [
    "for epoch_num in range(1, num_epochs + 1):\n",
    "    running_loss = 0.0\n",
    "    running_acc = 0.0\n",
    "\n",
    "    for inputs, labels in loaders[\"train\"]:\n",
    "        inputs, labels = inputs.view([batch_size, 28 ** 2]), labels.data\n",
    "\n",
    "        # Call our MPS to get logit scores and predictions\n",
    "        scores = mps(inputs)\n",
    "        _, preds = torch.max(scores, 1)\n",
    "\n",
    "        # Compute the loss and accuracy, add them to the running totals\n",
    "        loss = loss_fun(scores, labels)\n",
    "        with torch.no_grad():\n",
    "            accuracy = torch.sum(preds == labels).item() / batch_size\n",
    "            running_loss += loss\n",
    "            running_acc += accuracy\n",
    "\n",
    "        # Backpropagate and update parameters\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f\"### Epoch {epoch_num} ###\")\n",
    "    print(f\"Average loss:           {running_loss / num_batches['train']:.4f}\")\n",
    "    print(f\"Average train accuracy: {running_acc / num_batches['train']:.4f}\")\n",
    "\n",
    "    # Evaluate accuracy of MPS classifier on the test set\n",
    "    with torch.no_grad():\n",
    "        running_acc = 0.0\n",
    "\n",
    "        for inputs, labels in loaders[\"test\"]:\n",
    "            inputs, labels = inputs.view([batch_size, 28 ** 2]), labels.data\n",
    "\n",
    "            # Call our MPS to get logit scores and predictions\n",
    "            scores = mps(inputs)\n",
    "            _, preds = torch.max(scores, 1)\n",
    "            running_acc += torch.sum(preds == labels).item() / batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}